% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/knn.R
\name{fastknn}
\alias{fastknn}
\title{Fast KNN Classifier}
\usage{
fastknn(xtr, ytr, xte, k, method = "dist", normalize = NULL)
}
\arguments{
\item{xtr}{matrix containing the training instances. Rows are observations 
and columns are variables. Only numeric variables are allowed.}

\item{ytr}{factor array with the training labels.}

\item{xte}{matrix containing the test instances.}

\item{k}{number of neighbors considered.}

\item{method}{method used to infer the class membership probabilities of the 
test instances. Choose \code{"dist"} (default) to compute probabilites from 
the inverse of the nearest neighbor distances. This method works as
a shrinkage estimator and provides a better predictive performance in general.
Or you can choose \code{"vote"} to compute probabilities from the frequency 
of the nearest neighbor labels.}

\item{normalize}{variable normalization to be applied prior to searching the 
nearest neighbors. Default is \code{normalize=NULL}. Normalization is 
recommended if variables are not in the same units. It can be one of the 
following:
\itemize{
 \item{normalize="std"}: standardize variables by removing the mean and 
 scaling to unit variance. 
 \item{normalize="minmax"}: transforms variables by scaling each one between 
 0 and 1.
 \item{normalize="maxabs"}: scales each variable by its maximum absolute 
 value. This is the best choice for sparse data because it does not 
 shift/center the variables.
 \item{normalize="robust"}: scales variables using statistics that are 
 robust to outliers. It removes the median and scales by the interquartile 
 range (IQR).
}}
}
\value{
\code{list} with predictions for the test set:
\itemize{
 \item \code{class}: factor array of predicted classes.
 \item \code{prob}: matrix with predicted probabilities.
}
}
\description{
Fast k-Nearest Neighbor classifier build upon ANN, a high efficient 
\code{C++} library for nearest neighbor searching.
}
\details{
There are two estimators for the class membership probabilities:
\enumerate{
\item \code{metric="vote"}: The classical estimator based on the label 
proportions of the nearest neighbors. This estimator can be thought as of a 
\strong{voting} rule.
\item  \code{metric="dist"}: A shrinkage estimator based on the distances 
from the nearest neighbors, so that those neighbors more close to the test 
observation have more importance on predicting the class label. This 
estimator can be thought as of a \strong{weighted voting} rule. In general, 
it reduces log-loss.
}
}
\examples{
\dontrun{
library("mlbench")
library("caTools")
library("fastknn")

data("Ionosphere")

x <- data.matrix(subset(Ionosphere, select = -Class))
y <- Ionosphere$Class

set.seed(2048)
tr.idx <- which(sample.split(Y = y, SplitRatio = 0.7))
x.tr <- x[tr.idx,]
x.te <- x[-tr.idx,]
y.tr <- y[tr.idx]
y.te <- y[-tr.idx]

knn.out <- fastknn(xtr = x.tr, ytr = y.tr, xte = x.te, k = 10)

knn.out$class
knn.out$prob
}
}
\author{
David Pinto.
}

