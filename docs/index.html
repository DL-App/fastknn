<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Build Fast k-Nearest Neighbor Classifiers &bull; fastknn</title><!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous"><script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous"><!-- pkgdown --><link href="pkgdown.css" rel="stylesheet"><script src="jquery.sticky-kit.min.js"></script><script src="pkgdown.js"></script><!-- mathjax --><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--></head><body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">fastknn</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav"><li>
  <a href="reference/index.html">Reference</a>
</li>
<li>
  <a href="articles/index.html">Articles</a>
</li>
      </ul><ul class="nav navbar-nav navbar-right"><li>
  <a href="https://github.com/davpinto/fastknn">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul></div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    

    
    
<div class="contents">
<blockquote>
<p>Fast KNN with shrinkage estimator for the class membership probabilities</p>
</blockquote>
<p><a href="https://travis-ci.org/davpinto/fastknn"><img src="https://travis-ci.org/davpinto/fastknn.svg?branch=master" alt="Travis-CI Build Status"></a> <a href="https://cran.r-project.org/package=fastknn"><img src="http://www.r-pkg.org/badges/version/fastknn?color=blue" alt="CRAN_Status_Badge"></a></p>
<hr><p>The <code>fastknn</code> in now available on <a href="https://github.com/Kaggle/docker-rstats">Kaggle</a>. Take a look at this <a href="https://www.kaggle.com/davidpinto/d/uciml/forest-cover-type-dataset/fastknn-show-to-glm-what-knn-see-0-96">kernel</a> to see an example on how to use <code>fastknn</code> to improve your performance on <strong>Kaggle</strong> competitions.</p>
<hr><div id="why-fastknn" class="section level3">
<h3 class="hasAnchor"><a href="#why-fastknn" class="anchor"> </a>Why <code>fastknn</code>?</h3>
<ol style="list-style-type: decimal"><li>Build KNN classifiers with <strong>large datasets</strong> (&gt; 100k rows) in a few seconds.</li>
<li>Predict more <strong>calibrated probabilities</strong> and reduce log-loss with the <code>"dist"</code> estimator.</li>
<li>Find the <strong>best k</strong> parameter according to a variety of loss functions, using n-fold cross validation.</li>
<li>Plot beautiful classification <strong>decision boundaries</strong> for your dataset.</li>
<li>Do <strong>feature engineering</strong> and extract high informative features from your dataset.</li>
<li>Compete in <strong>Kaggle</strong>.</li>
</ol><p>Give it a try and let me know what you think!</p>
</div>
<div id="fast-nearest-neighbor-searching" class="section level2">
<h2 class="hasAnchor"><a href="#fast-nearest-neighbor-searching" class="anchor"> </a>Fast Nearest Neighbor Searching</h2>
<p>The <code>fastknn</code> method implements a k-Nearest Neighbor (KNN) classifier based on the <a href="https://www.cs.umd.edu/~mount/ANN">ANN</a> library. ANN is written in <code>C++</code> and is able to find the k nearest neighbors for every point in a given dataset in <code>O(N log N)</code> time. The package <a href="https://github.com/jefferis/RANN">RANN</a> provides an easy interface to use ANN library in <code>R</code>.</p>
</div>
<div id="the-fastknn-classifier" class="section level2">
<h2 class="hasAnchor"><a href="#the-fastknn-classifier" class="anchor"> </a>The FastKNN Classifier</h2>
<p>The <code>fastknn</code> was developed to deal with very large datasets (&gt; 100k rows) and is ideal to <a href="https://www.kaggle.com">Kaggle</a> competitions. It can be about 50x faster then the popular <code>knn</code> method from the <code>R</code> package <a href="https://cran.r-project.org/web/packages/class">class</a>, for large datasets. Moreover, <code>fastknn</code> provides a shrinkage estimator to the class membership probabilities, based on the inverse distances of the nearest neighbors (see the equations on <a href="https://davpinto.github.io/fastknn/">fastknn website</a>):</p>
<p><span class="math display">\[
P(x_i \in y_j) = \displaystyle\frac{\displaystyle\sum\limits_{k=1}^K \left( \frac{1}{d_{ik}}\cdot(n_{ik} \in y_j) \right)}{\displaystyle\sum\limits_{k=1}^K \left( \frac{1}{d_{ik}} \right)}
\]</span></p>
<p>where <span class="math inline">\(x_i\)</span> is the <span class="math inline">\(i^{\text{th}}\)</span> test instance, <span class="math inline">\(y_j\)</span> is the <span class="math inline">\(j^{\text{th}}\)</span> unique class label, <span class="math inline">\(n_{ik}\)</span> is the <span class="math inline">\(k^{\text{th}}\)</span> nearest neighbor of <span class="math inline">\(x_i\)</span>, and <span class="math inline">\(d_{ik}\)</span> is the distance between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(n_{ik}\)</span>. This estimator can be thought of as a weighted voting rule, where those neighbors that are more close to <span class="math inline">\(x_i\)</span> will have more influence on predicting <span class="math inline">\(x_i\)</span>&rsquo;s label.</p>
<p>In general, the weighted estimator provides more <strong>calibrated probabilities</strong> when compared with the traditional estimator based on the label proportions of the nearest neighbors, and reduces <strong>logarithmic loss</strong> (log-loss).</p>
<div id="how-to-install-fastknn" class="section level3">
<h3 class="hasAnchor"><a href="#how-to-install-fastknn" class="anchor"> </a>How to install <code>fastknn</code>?</h3>
<p>The package <code>fastknn</code> is not on CRAN, so you need to install it directly from GitHub:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">"devtools"</span>)
<span class="kw">install_github</span>(<span class="st">"davpinto/fastknn"</span>)</code></pre></div>
</div>
<div id="required-packages" class="section level3">
<h3 class="hasAnchor"><a href="#required-packages" class="anchor"> </a>Required Packages</h3>
<p>The base of <code>fastknn</code> is the <code>RANN</code> package, but other packages are required to make <code>fastknn</code> work properly. All of them are automatically installed when you install the <code>fastknn</code>.</p>
<ul><li><code>RANN</code> for fast nearest neighbors searching,</li>
<li><code>foreach</code> and <code>doSNOW</code> to do parallelized cross-validation,</li>
<li><code>Metrics</code> to measure classification performance,</li>
<li><code>matrixStats</code> for fast matrix column-wise and row-wise statistics,</li>
<li><code>ggplot2</code> to plot classification decision boundaries,</li>
<li><code>viridis</code> for modern color palletes.</li>
</ul></div>
<div id="getting-started" class="section level3">
<h3 class="hasAnchor"><a href="#getting-started" class="anchor"> </a>Getting Started</h3>
<p>Using <code>fastknn</code> is as simple as:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Load packages
<span class="kw">library</span>(<span class="st">"fastknn"</span>)
<span class="kw">library</span>(<span class="st">"caTools"</span>)

## Load toy data
<span class="kw">data</span>(<span class="st">"chess"</span>, <span class="dt">package =</span> <span class="st">"fastknn"</span>)

## Split data for training and test
<span class="kw">set.seed</span>(<span class="dv">123</span>)
tr.idx &lt;-<span class="st"> </span><span class="kw">which</span>(caTools::<span class="kw">sample.split</span>(<span class="dt">Y =</span> chess$y, <span class="dt">SplitRatio =</span> <span class="fl">0.7</span>))
x.tr   &lt;-<span class="st"> </span>chess$x[tr.idx, ]
x.te   &lt;-<span class="st"> </span>chess$x[-tr.idx, ]
y.tr   &lt;-<span class="st"> </span>chess$y[tr.idx]
y.te   &lt;-<span class="st"> </span>chess$y[-tr.idx]

## Fit KNN
yhat &lt;-<span class="st"> </span><span class="kw"><a href="reference/fastknn.html">fastknn</a></span>(x.tr, y.tr, x.te, <span class="dt">k =</span> <span class="dv">10</span>)

## Evaluate model on test set
<span class="kw">sprintf</span>(<span class="st">"Accuracy: %.2f"</span>, <span class="dv">100</span> *<span class="st"> </span>(<span class="dv">1</span> -<span class="st"> </span><span class="kw"><a href="reference/classLoss.html">classLoss</a></span>(<span class="dt">actual =</span> y.te, <span class="dt">predicted =</span> yhat$class)))</code></pre></div>
<pre><code>## [1] "Accuracy: 97.67"</code></pre>
</div>
</div>
<div id="find-the-best-k" class="section level2">
<h2 class="hasAnchor"><a href="#find-the-best-k" class="anchor"> </a>Find the Best k</h2>
<p>The <code>fastknn</code> provides a interface to select the best <code>k</code> using n-fold cross-validation. There are 4 possible <strong>loss functions</strong>:</p>
<ul><li>Overall classification error rate: <code>eval.metric = "overall_error"</code></li>
<li>Mean per-class classification error rate: <code>eval.metric = "mean_error"</code></li>
<li>Mean per-class AUC: <code>eval.metric = "auc"</code></li>
<li>Cross-entropy / logarithmic loss: <code>eval.metric = "logloss"</code></li>
</ul><p>Cross-validation using the <strong>voting</strong> probability estimator:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Load dataset
<span class="kw">library</span>(<span class="st">"mlbench"</span>)
<span class="kw">data</span>(<span class="st">"Sonar"</span>, <span class="dt">package =</span> <span class="st">"mlbench"</span>)
x &lt;-<span class="st"> </span><span class="kw">data.matrix</span>(Sonar[, -<span class="dv">61</span>])
y &lt;-<span class="st"> </span>Sonar$Class

## 5-fold CV using log-loss as evaluation metric
<span class="kw">set.seed</span>(<span class="dv">123</span>)
cv.out &lt;-<span class="st"> </span><span class="kw"><a href="reference/fastknnCV.html">fastknnCV</a></span>(x, y, <span class="dt">k =</span> <span class="dv">3</span>:<span class="dv">15</span>, <span class="dt">method =</span> <span class="st">"vote"</span>, <span class="dt">folds =</span> <span class="dv">5</span>, <span class="dt">eval.metric =</span> <span class="st">"logloss"</span>)
cv.out$cv_table</code></pre></div>
<table style="width:76%;" class="table"><colgroup><col width="12%"><col width="12%"><col width="12%"><col width="12%"><col width="12%"><col width="9%"><col width="4%"></colgroup><thead><tr class="header"><th align="center">fold_1</th>
<th align="center">fold_2</th>
<th align="center">fold_3</th>
<th align="center">fold_4</th>
<th align="center">fold_5</th>
<th align="center">mean</th>
<th align="center">k</th>
</tr></thead><tbody><tr class="odd"><td align="center">2.638</td>
<td align="center">3.629</td>
<td align="center">2.721</td>
<td align="center">1.895</td>
<td align="center">0.9809</td>
<td align="center">2.373</td>
<td align="center">3</td>
</tr><tr class="even"><td align="center">1.139</td>
<td align="center">2.079</td>
<td align="center">2.789</td>
<td align="center">1.104</td>
<td align="center">0.2251</td>
<td align="center">1.467</td>
<td align="center">4</td>
</tr><tr class="odd"><td align="center">1.203</td>
<td align="center">1.304</td>
<td align="center">2.791</td>
<td align="center">1.133</td>
<td align="center">0.315</td>
<td align="center">1.349</td>
<td align="center">5</td>
</tr><tr class="even"><td align="center">0.5285</td>
<td align="center">1.333</td>
<td align="center">2.011</td>
<td align="center">1.198</td>
<td align="center">0.358</td>
<td align="center">1.086</td>
<td align="center">6</td>
</tr><tr class="odd"><td align="center">0.5567</td>
<td align="center">0.5874</td>
<td align="center">2.031</td>
<td align="center">1.244</td>
<td align="center">0.3923</td>
<td align="center">0.9622</td>
<td align="center">7</td>
</tr><tr class="even"><td align="center">0.5657</td>
<td align="center">0.593</td>
<td align="center">2.058</td>
<td align="center">1.244</td>
<td align="center">0.417</td>
<td align="center">0.9755</td>
<td align="center">8</td>
</tr><tr class="odd"><td align="center">0.5502</td>
<td align="center">0.6228</td>
<td align="center">1.286</td>
<td align="center">0.4712</td>
<td align="center">0.4478</td>
<td align="center">0.6757</td>
<td align="center">9</td>
</tr><tr class="even"><td align="center">0.5864</td>
<td align="center">0.6344</td>
<td align="center">0.5025</td>
<td align="center">0.4843</td>
<td align="center">0.4854</td>
<td align="center">0.5386</td>
<td align="center">10</td>
</tr><tr class="odd"><td align="center">0.5975</td>
<td align="center">0.6518</td>
<td align="center">0.5116</td>
<td align="center">0.4765</td>
<td align="center">0.5134</td>
<td align="center">0.5502</td>
<td align="center">11</td>
</tr><tr class="even"><td align="center">0.6059</td>
<td align="center">0.6543</td>
<td align="center">0.5022</td>
<td align="center">0.4897</td>
<td align="center">0.5383</td>
<td align="center">0.5581</td>
<td align="center">12</td>
</tr><tr class="odd"><td align="center">0.5996</td>
<td align="center">0.6642</td>
<td align="center">0.5212</td>
<td align="center">0.5132</td>
<td align="center">0.566</td>
<td align="center">0.5728</td>
<td align="center">13</td>
</tr><tr class="even"><td align="center">0.6114</td>
<td align="center">0.6572</td>
<td align="center">0.5283</td>
<td align="center">0.5242</td>
<td align="center">0.5882</td>
<td align="center">0.5819</td>
<td align="center">14</td>
</tr><tr class="odd"><td align="center">0.6163</td>
<td align="center">0.6416</td>
<td align="center">0.5416</td>
<td align="center">0.5449</td>
<td align="center">0.5959</td>
<td align="center">0.5881</td>
<td align="center">15</td>
</tr></tbody></table><p>Cross-validation using the <strong>weighted voting</strong> probability estimator:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## 5-fold CV using log-loss as evaluation metric
<span class="kw">set.seed</span>(<span class="dv">123</span>)
cv.out &lt;-<span class="st"> </span><span class="kw"><a href="reference/fastknnCV.html">fastknnCV</a></span>(x, y, <span class="dt">k =</span> <span class="dv">3</span>:<span class="dv">15</span>, <span class="dt">method =</span> <span class="st">"dist"</span>, <span class="dt">folds =</span> <span class="dv">5</span>, <span class="dt">eval.metric =</span> <span class="st">"logloss"</span>)
cv.out$cv_table</code></pre></div>
<table style="width:76%;" class="table"><colgroup><col width="12%"><col width="12%"><col width="12%"><col width="12%"><col width="12%"><col width="9%"><col width="4%"></colgroup><thead><tr class="header"><th align="center">fold_1</th>
<th align="center">fold_2</th>
<th align="center">fold_3</th>
<th align="center">fold_4</th>
<th align="center">fold_5</th>
<th align="center">mean</th>
<th align="center">k</th>
</tr></thead><tbody><tr class="odd"><td align="center">2.626</td>
<td align="center">3.608</td>
<td align="center">2.707</td>
<td align="center">1.891</td>
<td align="center">0.9645</td>
<td align="center">2.359</td>
<td align="center">3</td>
</tr><tr class="even"><td align="center">1.111</td>
<td align="center">2.052</td>
<td align="center">2.766</td>
<td align="center">1.094</td>
<td align="center">0.1965</td>
<td align="center">1.444</td>
<td align="center">4</td>
</tr><tr class="odd"><td align="center">1.15</td>
<td align="center">1.263</td>
<td align="center">2.766</td>
<td align="center">1.112</td>
<td align="center">0.2682</td>
<td align="center">1.312</td>
<td align="center">5</td>
</tr><tr class="even"><td align="center">0.4569</td>
<td align="center">1.288</td>
<td align="center">1.987</td>
<td align="center">1.165</td>
<td align="center">0.2946</td>
<td align="center">1.038</td>
<td align="center">6</td>
</tr><tr class="odd"><td align="center">0.4715</td>
<td align="center">0.5304</td>
<td align="center">1.999</td>
<td align="center">1.199</td>
<td align="center">0.3192</td>
<td align="center">0.9039</td>
<td align="center">7</td>
</tr><tr class="even"><td align="center">0.4786</td>
<td align="center">0.5315</td>
<td align="center">2.022</td>
<td align="center">1.2</td>
<td align="center">0.3391</td>
<td align="center">0.9142</td>
<td align="center">8</td>
</tr><tr class="odd"><td align="center">0.4628</td>
<td align="center">0.5587</td>
<td align="center">1.246</td>
<td align="center">0.4257</td>
<td align="center">0.3636</td>
<td align="center">0.6114</td>
<td align="center">9</td>
</tr><tr class="even"><td align="center">0.4918</td>
<td align="center">0.5664</td>
<td align="center">0.4651</td>
<td align="center">0.4357</td>
<td align="center">0.3912</td>
<td align="center">0.47</td>
<td align="center">10</td>
</tr><tr class="odd"><td align="center">0.5002</td>
<td align="center">0.5783</td>
<td align="center">0.4686</td>
<td align="center">0.427</td>
<td align="center">0.415</td>
<td align="center">0.4778</td>
<td align="center">11</td>
</tr><tr class="even"><td align="center">0.5101</td>
<td align="center">0.5768</td>
<td align="center">0.4625</td>
<td align="center">0.4367</td>
<td align="center">0.4386</td>
<td align="center">0.485</td>
<td align="center">12</td>
</tr><tr class="odd"><td align="center">0.503</td>
<td align="center">0.5861</td>
<td align="center">0.4765</td>
<td align="center">0.4542</td>
<td align="center">0.4626</td>
<td align="center">0.4965</td>
<td align="center">13</td>
</tr><tr class="even"><td align="center">0.5116</td>
<td align="center">0.5794</td>
<td align="center">0.4826</td>
<td align="center">0.4663</td>
<td align="center">0.4836</td>
<td align="center">0.5047</td>
<td align="center">14</td>
</tr><tr class="odd"><td align="center">0.5194</td>
<td align="center">0.5742</td>
<td align="center">0.4938</td>
<td align="center">0.4842</td>
<td align="center">0.4926</td>
<td align="center">0.5128</td>
<td align="center">15</td>
</tr></tbody></table><p>Note that the mean <strong>log-loss</strong> for the <strong>weighted voting</strong> estimator is lower for every <code>k</code> evaluated.</p>
<p>Parallelization is available. You can specify the number of threads via <code>nthread</code> parameter.</p>
</div>
<div id="plot-classification-decision-boundary" class="section level2">
<h2 class="hasAnchor"><a href="#plot-classification-decision-boundary" class="anchor"> </a>Plot Classification Decision Boundary</h2>
<p>The <code>fastknn</code> provides a plotting function, based on <code>ggplot2</code>, to draw bi-dimensional decision boundaries. If your dataset has more than 2 variables, only the first two will be considered. In future versions of <code>fastknn</code> the most descriptive variables will be selected automatically beforehand, using a <strong>feature ranking</strong> technique.</p>
<div id="two-class-problem" class="section level3">
<h3 class="hasAnchor"><a href="#two-class-problem" class="anchor"> </a>Two-class Problem</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Load toy data
<span class="kw">data</span>(<span class="st">"spirals"</span>, <span class="dt">package =</span> <span class="st">"fastknn"</span>)

## Split data for training and test
<span class="kw">set.seed</span>(<span class="dv">123</span>)
tr.idx &lt;-<span class="st"> </span><span class="kw">which</span>(caTools::<span class="kw">sample.split</span>(<span class="dt">Y =</span> spirals$y, <span class="dt">SplitRatio =</span> <span class="fl">0.7</span>))
x.tr   &lt;-<span class="st"> </span>spirals$x[tr.idx, ]
x.te   &lt;-<span class="st"> </span>spirals$x[-tr.idx, ]
y.tr   &lt;-<span class="st"> </span>spirals$y[tr.idx]
y.te   &lt;-<span class="st"> </span>spirals$y[-tr.idx]

## Plot decision boundary
<span class="kw"><a href="reference/knnDecision.html">knnDecision</a></span>(x.tr, y.tr, x.te, y.te, <span class="dt">k =</span> <span class="dv">15</span>)</code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-7-1.png" width="840" style="display: block; margin: auto;"></p>
</div>
<div id="multi-class-problem" class="section level3">
<h3 class="hasAnchor"><a href="#multi-class-problem" class="anchor"> </a>Multi-class Problem</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Load toy data
<span class="kw">data</span>(<span class="st">"multi_spirals"</span>, <span class="dt">package =</span> <span class="st">"fastknn"</span>)

## Split data for training and test
<span class="kw">set.seed</span>(<span class="dv">123</span>)
tr.idx &lt;-<span class="st"> </span><span class="kw">which</span>(caTools::<span class="kw">sample.split</span>(<span class="dt">Y =</span> multi_spirals$y, <span class="dt">SplitRatio =</span> <span class="fl">0.7</span>))
x.tr   &lt;-<span class="st"> </span>multi_spirals$x[tr.idx, ]
x.te   &lt;-<span class="st"> </span>multi_spirals$x[-tr.idx, ]
y.tr   &lt;-<span class="st"> </span>multi_spirals$y[tr.idx]
y.te   &lt;-<span class="st"> </span>multi_spirals$y[-tr.idx]

## Plot decision boundary
<span class="kw"><a href="reference/knnDecision.html">knnDecision</a></span>(x.tr, y.tr, x.te, y.te, <span class="dt">k =</span> <span class="dv">15</span>)</code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-8-1.png" width="840" style="display: block; margin: auto;"></p>
</div>
</div>
<div id="performance-test" class="section level2">
<h2 class="hasAnchor"><a href="#performance-test" class="anchor"> </a>Performance Test</h2>
<p>Here we test the performance of <code>fastknn</code> on the <a href="https://archive.ics.uci.edu/ml/datasets/Covertype">Covertype</a> datset. It is hosted on <a href="https://archive.ics.uci.edu/ml/">UCI</a> repository and has been already used in a <strong>Kaggle</strong> <a href="https://www.kaggle.com/c/forest-cover-type-prediction">competition</a>. The dataset contains 581012 observations on 54 numeric features, classified into 7 different categories.</p>
<p>All experiments were conducted on a <strong>64-bit Ubuntu 16.04 with Intel Core i7-6700HQ 2.60GHz and 16GB RAM DDR4</strong>.</p>
<div id="computing-time" class="section level3">
<h3 class="hasAnchor"><a href="#computing-time" class="anchor"> </a>Computing Time</h3>
<p>Here <code>fastknn</code> is compared with the <code>knn</code> method from the package <code>class</code>. We had to use small samples from the Covertype data because <code>knn</code> takes too much time (&gt; 1500s) to fit the entire dataset.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">#### Load packages
<span class="kw">library</span>(<span class="st">'class'</span>)
<span class="kw">library</span>(<span class="st">'fastknn'</span>)
<span class="kw">library</span>(<span class="st">'caTools'</span>)

#### Load data
<span class="kw">data</span>(<span class="st">"covertype"</span>, <span class="dt">package =</span> <span class="st">"fastknn"</span>)
covertype$Target &lt;-<span class="st"> </span><span class="kw">as.factor</span>(covertype$Target)

#### Test with different sample sizes
N &lt;-<span class="st"> </span><span class="kw">nrow</span>(covertype)
sample.frac &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">10e3</span>, <span class="fl">15e3</span>, <span class="fl">20e3</span>)/N
res &lt;-<span class="st"> </span><span class="kw">lapply</span>(sample.frac, function(frac, dt) {
   ## Reduce datset
   <span class="kw">set.seed</span>(<span class="dv">123</span>)
   sample.idx &lt;-<span class="st"> </span><span class="kw">which</span>(<span class="kw">sample.split</span>(dt$Target, <span class="dt">SplitRatio =</span> frac))
   x &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(dt[sample.idx, -<span class="dv">55</span>])
   y &lt;-<span class="st"> </span>dt$Target[sample.idx]
   
   ## Split data
   <span class="kw">set.seed</span>(<span class="dv">123</span>)
   tr.idx &lt;-<span class="st"> </span><span class="kw">which</span>(<span class="kw">sample.split</span>(y, <span class="dt">SplitRatio =</span> <span class="fl">0.7</span>))
   x.tr   &lt;-<span class="st"> </span>x[tr.idx, ]
   x.te   &lt;-<span class="st"> </span>x[-tr.idx, ]
   y.tr   &lt;-<span class="st"> </span>y[tr.idx]
   y.te   &lt;-<span class="st"> </span>y[-tr.idx]
   
   ## Measure time
   t1 &lt;-<span class="st"> </span><span class="kw">system.time</span>({
      yhat1 &lt;-<span class="st"> </span><span class="kw">knn</span>(<span class="dt">train =</span> x.tr, <span class="dt">test =</span> x.te, <span class="dt">cl =</span> y.tr, <span class="dt">k =</span> <span class="dv">10</span>, <span class="dt">prob =</span> <span class="ot">TRUE</span>)
   })
   t2 &lt;-<span class="st"> </span><span class="kw">system.time</span>({
      yhat2 &lt;-<span class="st"> </span><span class="kw"><a href="reference/fastknn.html">fastknn</a></span>(<span class="dt">xtr =</span> x.tr, <span class="dt">ytr =</span> y.tr, <span class="dt">xte =</span> x.te, <span class="dt">k =</span> <span class="dv">10</span>, <span class="dt">method =</span> <span class="st">"dist"</span>)
   })
   
   ## Return
   <span class="kw">list</span>(
      <span class="dt">method =</span> <span class="kw">c</span>(<span class="st">'knn'</span>, <span class="st">'fastknn'</span>),
      <span class="dt">nobs =</span> <span class="kw">as.integer</span>(<span class="kw">rep</span>(N*frac, <span class="dv">2</span>)),
      <span class="dt">time_sec =</span> <span class="kw">c</span>(t1[<span class="dv">3</span>], t2[<span class="dv">3</span>]), 
      <span class="dt">accuracy =</span> <span class="kw">round</span>(<span class="dv">100</span> *<span class="st"> </span><span class="kw">c</span>(<span class="kw">sum</span>(yhat1 ==<span class="st"> </span>y.te), <span class="kw">sum</span>(yhat2$class ==<span class="st"> </span>y.te)) /<span class="st"> </span><span class="kw">length</span>(y.te), <span class="dv">2</span>)
   )
}, <span class="dt">dt =</span> covertype)
res &lt;-<span class="st"> </span><span class="kw">do.call</span>(<span class="st">'rbind.data.frame'</span>, res)
res</code></pre></div>
<table style="width:53%;" class="table"><colgroup><col width="12%"><col width="9%"><col width="15%"><col width="15%"></colgroup><thead><tr class="header"><th align="center">method</th>
<th align="center">nobs</th>
<th align="center">time_sec</th>
<th align="center">accuracy</th>
</tr></thead><tbody><tr class="odd"><td align="center">knn</td>
<td align="center">10000</td>
<td align="center">1.291</td>
<td align="center">73.38</td>
</tr><tr class="even"><td align="center">fastknn</td>
<td align="center">10000</td>
<td align="center">0.152</td>
<td align="center">77.04</td>
</tr><tr class="odd"><td align="center">knn</td>
<td align="center">15000</td>
<td align="center">3.018</td>
<td align="center">73.71</td>
</tr><tr class="even"><td align="center">fastknn</td>
<td align="center">15000</td>
<td align="center">0.18</td>
<td align="center">76.82</td>
</tr><tr class="odd"><td align="center">knn</td>
<td align="center">20000</td>
<td align="center">6.381</td>
<td align="center">76.38</td>
</tr><tr class="even"><td align="center">fastknn</td>
<td align="center">20000</td>
<td align="center">0.205</td>
<td align="center">80.37</td>
</tr></tbody></table><p>The <code>fastknn</code> takes <strong>about 5s</strong> to fit the entire dataset.</p>
</div>
<div id="probability-prediction" class="section level3">
<h3 class="hasAnchor"><a href="#probability-prediction" class="anchor"> </a>Probability Prediction</h3>
<p>We compared the <code>voting</code> estimator with the <code>weighted voting</code> estimator:</p>
<p><strong>Voting</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">#### Extract input variables and response variable
x &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(covertype[, -<span class="dv">55</span>])
y &lt;-<span class="st"> </span><span class="kw">as.factor</span>(covertype$Target)

#### 5-fold cross-validation
<span class="kw">set.seed</span>(<span class="dv">123</span>)
res &lt;-<span class="st"> </span><span class="kw"><a href="reference/fastknnCV.html">fastknnCV</a></span>(x, y, <span class="dt">k =</span> <span class="dv">10</span>, <span class="dt">method =</span> <span class="st">"vote"</span>, <span class="dt">folds =</span> <span class="dv">5</span>, <span class="dt">eval.metric =</span> <span class="st">"logloss"</span>)
res$cv_table</code></pre></div>
<table style="width:76%;" class="table"><colgroup><col width="12%"><col width="12%"><col width="12%"><col width="12%"><col width="12%"><col width="9%"><col width="4%"></colgroup><thead><tr class="header"><th align="center">fold_1</th>
<th align="center">fold_2</th>
<th align="center">fold_3</th>
<th align="center">fold_4</th>
<th align="center">fold_5</th>
<th align="center">mean</th>
<th align="center">k</th>
</tr></thead><tbody><tr class="odd"><td align="center">0.6081</td>
<td align="center">0.5524</td>
<td align="center">0.5643</td>
<td align="center">0.5682</td>
<td align="center">0.6528</td>
<td align="center">0.5892</td>
<td align="center">10</td>
</tr></tbody></table><p><strong>Weighted Voting</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">#### 5-fold cross-validation
<span class="kw">set.seed</span>(<span class="dv">123</span>)
res &lt;-<span class="st"> </span><span class="kw"><a href="reference/fastknnCV.html">fastknnCV</a></span>(x, y, <span class="dt">k =</span> <span class="dv">10</span>, <span class="dt">method =</span> <span class="st">"dist"</span>, <span class="dt">folds =</span> <span class="dv">5</span>, <span class="dt">eval.metric =</span> <span class="st">"logloss"</span>)
res$cv_table</code></pre></div>
<table style="width:76%;" class="table"><colgroup><col width="12%"><col width="12%"><col width="12%"><col width="12%"><col width="12%"><col width="9%"><col width="4%"></colgroup><thead><tr class="header"><th align="center">fold_1</th>
<th align="center">fold_2</th>
<th align="center">fold_3</th>
<th align="center">fold_4</th>
<th align="center">fold_5</th>
<th align="center">mean</th>
<th align="center">k</th>
</tr></thead><tbody><tr class="odd"><td align="center">0.5586</td>
<td align="center">0.5039</td>
<td align="center">0.5176</td>
<td align="center">0.5181</td>
<td align="center">0.604</td>
<td align="center">0.5404</td>
<td align="center">10</td>
</tr></tbody></table></div>
</div>
<div id="feature-engineering" class="section level2">
<h2 class="hasAnchor"><a href="#feature-engineering" class="anchor"> </a>Feature Engineering</h2>
<p>The <strong>fastknn</strong> provides a function to do <strong>feature extraction</strong> using KNN. It generates <code>k * c</code> new features, where <code>c</code> is the number of class labels. The new features are computed from the distances between the observations and their <code>k</code> nearest neighbors inside each class. The following example shows that the <strong>KNN features</strong> carry information that can not be extracted from data by a linear learner, like a GLM model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">"mlbench"</span>)
<span class="kw">library</span>(<span class="st">"caTools"</span>)
<span class="kw">library</span>(<span class="st">"fastknn"</span>)
<span class="kw">library</span>(<span class="st">"glmnet"</span>)

#### Load data
<span class="kw">data</span>(<span class="st">"Ionosphere"</span>, <span class="dt">package =</span> <span class="st">"mlbench"</span>)
x &lt;-<span class="st"> </span><span class="kw">data.matrix</span>(<span class="kw">subset</span>(Ionosphere, <span class="dt">select =</span> -Class))
y &lt;-<span class="st"> </span>Ionosphere$Class

#### Remove near zero variance columns
x &lt;-<span class="st"> </span>x[, -<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)]

#### Split data
<span class="kw">set.seed</span>(<span class="dv">123</span>)
tr.idx &lt;-<span class="st"> </span><span class="kw">which</span>(<span class="kw">sample.split</span>(<span class="dt">Y =</span> y, <span class="dt">SplitRatio =</span> <span class="fl">0.7</span>))
x.tr &lt;-<span class="st"> </span>x[tr.idx,]
x.te &lt;-<span class="st"> </span>x[-tr.idx,]
y.tr &lt;-<span class="st"> </span>y[tr.idx]
y.te &lt;-<span class="st"> </span>y[-tr.idx]

#### GLM with original features
glm &lt;-<span class="st"> </span><span class="kw">glmnet</span>(<span class="dt">x =</span> x.tr, <span class="dt">y =</span> y.tr, <span class="dt">family =</span> <span class="st">"binomial"</span>, <span class="dt">lambda =</span> <span class="dv">0</span>)
yhat &lt;-<span class="st"> </span><span class="kw">drop</span>(<span class="kw">predict</span>(glm, x.te, <span class="dt">type =</span> <span class="st">"class"</span>))
yhat1 &lt;-<span class="st"> </span><span class="kw">factor</span>(yhat, <span class="dt">levels =</span> <span class="kw">levels</span>(y.tr))

#### Generate KNN features
<span class="kw">set.seed</span>(<span class="dv">123</span>)
new.data &lt;-<span class="st"> </span><span class="kw"><a href="reference/knnExtract.html">knnExtract</a></span>(<span class="dt">xtr =</span> x.tr, <span class="dt">ytr =</span> y.tr, <span class="dt">xte =</span> x.te, <span class="dt">k =</span> <span class="dv">3</span>)

#### GLM with KNN features
glm &lt;-<span class="st"> </span><span class="kw">glmnet</span>(<span class="dt">x =</span> new.data$new.tr, <span class="dt">y =</span> y.tr, <span class="dt">family =</span> <span class="st">"binomial"</span>, <span class="dt">lambda =</span> <span class="dv">0</span>)
yhat &lt;-<span class="st"> </span><span class="kw">drop</span>(<span class="kw">predict</span>(glm, new.data$new.te, <span class="dt">type =</span> <span class="st">"class"</span>))
yhat2 &lt;-<span class="st"> </span><span class="kw">factor</span>(yhat, <span class="dt">levels =</span> <span class="kw">levels</span>(y.tr))

#### Performance
<span class="kw">sprintf</span>(<span class="st">"Accuracy: %.2f"</span>, <span class="dv">100</span> *<span class="st"> </span>(<span class="dv">1</span> -<span class="st"> </span><span class="kw"><a href="reference/classLoss.html">classLoss</a></span>(<span class="dt">actual =</span> y.te, <span class="dt">predicted =</span> yhat1)))
<span class="kw">sprintf</span>(<span class="st">"Accuracy: %.2f"</span>, <span class="dv">100</span> *<span class="st"> </span>(<span class="dv">1</span> -<span class="st"> </span><span class="kw"><a href="reference/classLoss.html">classLoss</a></span>(<span class="dt">actual =</span> y.te, <span class="dt">predicted =</span> yhat2)))</code></pre></div>
<pre><code>## [1] "Accuracy: 83.81"</code></pre>
<pre><code>## [1] "Accuracy: 95.24"</code></pre>
<p>We can see that the <strong>KNN features</strong> improved a lot the classification performance of the GLM model.</p>
<p>The <code><a href="reference/knnExtract.html">knnExtract()</a></code> function is based on the ideas presented in the<br><a href="https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14335/1st-place-winner-solution-gilberto-titericz-stanislav-semenov">winner solution</a> of the <a href="https://www.kaggle.com/c/otto-group-product-classification-challenge">Otto Group Product Classification Challenge</a> on <strong>Kaggle</strong>.</p>
<p>Parallelization is available. You can specify the number of threads via <code>nthread</code> parameter.</p>
<div id="understanding-the-knn-features" class="section level3">
<h3 class="hasAnchor"><a href="#understanding-the-knn-features" class="anchor"> </a>Understanding the KNN Features</h3>
<p>KNN makes a nonlinear mapping of the original space and project it into a linear one, in which the classes are linearly separable.</p>
<p><strong>Mapping the <em>chess</em> dataset</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">"caTools"</span>)
<span class="kw">library</span>(<span class="st">"fastknn"</span>)
<span class="kw">library</span>(<span class="st">"ggplot2"</span>)
<span class="kw">library</span>(<span class="st">"gridExtra"</span>)

## Load data
<span class="kw">data</span>(<span class="st">"chess"</span>)
x &lt;-<span class="st"> </span><span class="kw">data.matrix</span>(chess$x)
y &lt;-<span class="st"> </span>chess$y

## Split data
<span class="kw">set.seed</span>(<span class="dv">123</span>)
tr.idx &lt;-<span class="st"> </span><span class="kw">which</span>(<span class="kw">sample.split</span>(<span class="dt">Y =</span> y, <span class="dt">SplitRatio =</span> <span class="fl">0.7</span>))
x.tr &lt;-<span class="st"> </span>x[tr.idx,]
x.te &lt;-<span class="st"> </span>x[-tr.idx,]
y.tr &lt;-<span class="st"> </span>y[tr.idx]
y.te &lt;-<span class="st"> </span>y[-tr.idx]

## Feature extraction with KNN
<span class="kw">set.seed</span>(<span class="dv">123</span>)
new.data &lt;-<span class="st"> </span><span class="kw"><a href="reference/knnExtract.html">knnExtract</a></span>(x.tr, y.tr, x.te, <span class="dt">k =</span> <span class="dv">1</span>)

## Decision boundaries
g1 &lt;-<span class="st"> </span><span class="kw"><a href="reference/knnDecision.html">knnDecision</a></span>(x.tr, y.tr, x.te, y.te, <span class="dt">k =</span> <span class="dv">10</span>) +
<span class="st">   </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">"Original Features"</span>)
g2 &lt;-<span class="st"> </span><span class="kw"><a href="reference/knnDecision.html">knnDecision</a></span>(new.data$new.tr, y.tr, new.data$new.te, y.te, <span class="dt">k =</span> <span class="dv">10</span>) +
<span class="st">   </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">"KNN Features"</span>)
<span class="kw">grid.arrange</span>(g1, g2, <span class="dt">ncol =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-17-1.png" width="840" style="display: block; margin: auto;"></p>
<p><strong>Mapping the <em>spirals</em> dataset</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Load data
<span class="kw">data</span>(<span class="st">"spirals"</span>)
x &lt;-<span class="st"> </span><span class="kw">data.matrix</span>(spirals$x)
y &lt;-<span class="st"> </span>spirals$y

## Split data
<span class="kw">set.seed</span>(<span class="dv">123</span>)
tr.idx &lt;-<span class="st"> </span><span class="kw">which</span>(<span class="kw">sample.split</span>(<span class="dt">Y =</span> y, <span class="dt">SplitRatio =</span> <span class="fl">0.7</span>))
x.tr &lt;-<span class="st"> </span>x[tr.idx,]
x.te &lt;-<span class="st"> </span>x[-tr.idx,]
y.tr &lt;-<span class="st"> </span>y[tr.idx]
y.te &lt;-<span class="st"> </span>y[-tr.idx]

## Feature extraction with KNN
<span class="kw">set.seed</span>(<span class="dv">123</span>)
new.data &lt;-<span class="st"> </span><span class="kw"><a href="reference/knnExtract.html">knnExtract</a></span>(x.tr, y.tr, x.te, <span class="dt">k =</span> <span class="dv">1</span>)

## Decision boundaries
g1 &lt;-<span class="st"> </span><span class="kw"><a href="reference/knnDecision.html">knnDecision</a></span>(x.tr, y.tr, x.te, y.te, <span class="dt">k =</span> <span class="dv">10</span>) +
<span class="st">   </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">"Original Features"</span>)
g2 &lt;-<span class="st"> </span><span class="kw"><a href="reference/knnDecision.html">knnDecision</a></span>(new.data$new.tr, y.tr, new.data$new.te, y.te, <span class="dt">k =</span> <span class="dv">10</span>) +
<span class="st">   </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">"KNN Features"</span>)
<span class="kw">grid.arrange</span>(g1, g2, <span class="dt">ncol =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="index_files/figure-html/unnamed-chunk-18-1.png" width="840" style="display: block; margin: auto;"></p>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
    <h2>Links</h2><ul class="list-unstyled"><li>Browse source code at <br><a href="https://github.com/davpinto/fastknn">https://&#8203;github.com/&#8203;davpinto/&#8203;fastknn</a></li>
<li>Report a bug at <br><a href="https://github.com/davpinto/fastknn/issues">https://&#8203;github.com/&#8203;davpinto/&#8203;fastknn/&#8203;issues</a></li>
</ul><h2>License</h2>
<p><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></p>
<h2>Developers</h2><ul class="list-unstyled"><li>David Pinto <br><small class="roles"> Author, maintainer </small> </li>
</ul></div>

</div>


      <footer><div class="copyright">
  <p>Developed by David Pinto.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer></div>

  </body></html>
